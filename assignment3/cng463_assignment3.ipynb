{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh3Qjh8mRHqD"
      },
      "source": [
        "# Assignment 3: Hidden Markov Models for POS Tagging and Text Generation\n",
        "## CNG463 - Introduction to Natural Language Processing\n",
        "### METU NCC Computer Engineering | Fall 2025-26\n",
        "\n",
        "**Student Name:**  \n",
        "**Student ID:**  \n",
        "**Due Date:** 14 December 2025 (Sunday) before midnight\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyDEnwcxRHqE"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This assignment focuses on:\n",
        "1. Building **supervised**, **unsupervised**, and **semi-supervised** HMM models for Part-of-Speech (POS) tagging\n",
        "2. Implementing **5-fold cross-validation** to evaluate model performance\n",
        "3. Comparing the three training approaches using per-tag and overall accuracy\n",
        "4. Using HMMs for **creative text generation** with temperature-based sampling\n",
        "\n",
        "**Note:** You will use the Brown corpus with universal POS tagset. Start with 5000 sentences for debugging, then run on the full corpus (or as much as Colab can handle).\n",
        "\n",
        "**Grading:**\n",
        "- Helper Functions: **10 pts**\n",
        "  - `remove_tags()`: 2 pts\n",
        "  - `evaluate_tagger()`: 5 pts\n",
        "  - Results display: 3 pts\n",
        "- Semi-supervised HMM: **20 pts**\n",
        "- 5-fold cross-validation: **25 pts**\n",
        "  - Supervised HMM: 10 pts\n",
        "  - Unsupervised HMM (Baum-Welch): 10 pts\n",
        "  - Semi-supervised HMM\n",
        "  - Evaluate and Accumulate Results: 5 pts\n",
        "- Report on Results: **5 pts**\n",
        "- Text Generation: **24 pts**\n",
        "  - `sample_state()`: 7 pts\n",
        "  - `sample_word()`: 7 pts\n",
        "  - `generate_text_from_word()`: 10 pts\n",
        "- Written Questions (4 × 4 pts): **16 pts**\n",
        "- **Total: 100 pts**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srF42PZXRHqP"
      },
      "source": [
        "---\n",
        "\n",
        "## Pre-Submission Checklist\n",
        "\n",
        "- [ ] Name and student ID at top\n",
        "- [ ] No cells are added or removed\n",
        "- [ ] All TODO sections completed\n",
        "- [ ] All questions answered\n",
        "- [ ] Code runs without errors\n",
        "- [ ] Results tables included\n",
        "- [ ] Run All before saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq0Xw3YQRHqE"
      },
      "source": [
        "## Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gwyLNfKaRHqE"
      },
      "outputs": [],
      "source": [
        "# Standard libraries\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "# NLTK for corpus and HMM\n",
        "import nltk\n",
        "from nltk.tag import hmm\n",
        "\n",
        "# Scikit-learn for cross-validation\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mr-UNnAGRHqF"
      },
      "source": [
        "---\n",
        "\n",
        "# Task 1: HMM-based POS Tagging (72 points)\n",
        "\n",
        "In this part, you will implement three different approaches to training HMM models for POS tagging:\n",
        "1. **Supervised learning**: Uses fully labelled data\n",
        "2. **Unsupervised learning**: Uses unlabelled data with Baum-Welch algorithm\n",
        "3. **Semi-supervised learning**: Combines a small amount of labelled data with larger unlabelled data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5k13c4MRHqF"
      },
      "source": [
        "## 1.1: Load the Brown Corpus\n",
        "\n",
        "Load the Brown corpus with universal POS tagset. Start with 5000 sentences for testing, then increase to the maximum your environment can handle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cb6tjNLmRHqF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74f6d571-b3aa-4807-a100-52c321077d18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sentences loaded: 5000\n",
            "\n",
            "Example sentence:\n",
            "[('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN'), ('said', 'VERB'), ('Friday', 'NOUN'), ('an', 'DET'), ('investigation', 'NOUN'), ('of', 'ADP'), (\"Atlanta's\", 'NOUN'), ('recent', 'ADJ'), ('primary', 'NOUN'), ('election', 'NOUN'), ('produced', 'VERB'), ('``', '.'), ('no', 'DET'), ('evidence', 'NOUN'), (\"''\", '.'), ('that', 'ADP'), ('any', 'DET'), ('irregularities', 'NOUN'), ('took', 'VERB'), ('place', 'NOUN'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "# Download required NLTK data\n",
        "from nltk.corpus import brown\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "NUM_SENTENCES = 5000  # Start with 5000 sentences and increase later\n",
        "\n",
        "all_data = list(brown.tagged_sents(tagset=\"universal\"))[:5000]\n",
        "\n",
        "print(f\"Total sentences loaded: {len(all_data)}\")\n",
        "print(f\"\\nExample sentence:\")\n",
        "print(all_data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwDDP9gORHqF"
      },
      "source": [
        "## 1.2: Remove Tags (2 points)\n",
        "\n",
        "Implement the `remove_tags()` function that converts tagged sentences to untagged format required by NLTK's unsupervised training.\n",
        "\n",
        "**Input:** List of tagged sentences `[[(word, tag), ...], ...]`  \n",
        "**Output:** List of untagged sentences `[[(word, None), ...], ...]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FI_9lAXTRHqF"
      },
      "outputs": [],
      "source": [
        "def remove_tags(tagged_sents):\n",
        "    \"\"\"\n",
        "    Remove tags from tagged sentences to create untagged data.\n",
        "\n",
        "    Args:\n",
        "        tagged_sents: List of tagged sentences [[(word, tag), ...], ...]\n",
        "\n",
        "    Returns:\n",
        "        List of untagged sentences [[(word, None), ...], ...] (required format for NLTK)\n",
        "    \"\"\"\n",
        "    # TODO: Implement this function\n",
        "\n",
        "    untagged_data = []\n",
        "\n",
        "    # 2. Loop through every sentence in the input data\n",
        "    for sentence in tagged_sents:\n",
        "\n",
        "        # Create a temporary list for the current sentence\n",
        "        new_sentence = []\n",
        "\n",
        "        # 3. Loop through every word pair in the current sentence\n",
        "        for word, tag in sentence:\n",
        "            # Keep the word, but change the tag to None\n",
        "            new_pair = (word, None)\n",
        "            new_sentence.append(new_pair)\n",
        "\n",
        "        # Add the processed sentence to our main list\n",
        "        untagged_data.append(new_sentence)\n",
        "\n",
        "    return untagged_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mg18ezdTRHqG"
      },
      "source": [
        "## 1.3: Evaluate Tagger (5 points)\n",
        "\n",
        "Implement the `evaluate_tagger()` function that evaluates a trained tagger on test data and returns per-tag accuracy statistics.\n",
        "\n",
        "**Input:**\n",
        "- `tagger`: Trained HMM tagger\n",
        "- `test_data`: List of tagged test sentences\n",
        "\n",
        "**Output:** Dictionary with per-tag statistics `{tag: {'correct': count, 'total': count}}`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cLjQTbejRHqG"
      },
      "outputs": [],
      "source": [
        "def evaluate_tagger(tagger, test_data):\n",
        "    \"\"\"\n",
        "    Evaluate tagger and return per-tag accuracy.\n",
        "\n",
        "    Args:\n",
        "        tagger: Trained HMM tagger\n",
        "        test_data: List of tagged sentences for testing\n",
        "\n",
        "    Returns:\n",
        "        dict of {tag: {'correct': count, 'total': count}}\n",
        "    \"\"\"\n",
        "    # TODO: Implement this function\n",
        "    #\n",
        "    # Steps:\n",
        "    # 1. Initialize tag_stats as defaultdict(lambda: {'correct': 0, 'total': 0})\n",
        "    # 2. For each sentence in test_data:\n",
        "    #    a. Extract words and true tags\n",
        "    #    b. Use tagger.tag(words) to get predicted tags\n",
        "    #    c. Compare true vs predicted tags\n",
        "    #    d. Update tag_stats accordingly\n",
        "    # 3. Handle exceptions (if tagging fails, count all as incorrect)\n",
        "    # 4. Return tag_stats\n",
        "\n",
        "\n",
        "def evaluate_tagger(tagger, test_data):\n",
        "    \"\"\"\n",
        "    Evaluate tagger and return per-tag accuracy.\n",
        "    \"\"\"\n",
        "    # 1. Initialize stats container\n",
        "    # When we access a new tag, it automatically starts with 0 correct and 0 total\n",
        "    tag_stats = defaultdict(lambda: {'correct': 0, 'total': 0})\n",
        "\n",
        "    # 2. Loop through each sentence in the test data\n",
        "    for sentence in test_data:\n",
        "        # Separate the words from the tags\n",
        "        # sentence input: [('The', 'DET'), ('cat', 'NOUN')]\n",
        "        # words: ['The', 'cat']\n",
        "        # true_tags: ['DET', 'NOUN']\n",
        "        words = [word for word, tag in sentence]\n",
        "        true_tags = [tag for word, tag in sentence]\n",
        "\n",
        "        try:\n",
        "            # b. Use tagger.tag(words) to get predictions\n",
        "            # The tagger returns a list of tuples: [('The', 'DET'), ('cat', 'VERB')]\n",
        "            predicted_pairs = tagger.tag(words)\n",
        "\n",
        "            # Extract just the tags from the prediction\n",
        "            predicted_tags = [tag for word, tag in predicted_pairs]\n",
        "\n",
        "            # c. Compare true vs predicted tags\n",
        "            # zip() allows us to loop through both lists at the same time\n",
        "            for true_tag, pred_tag in zip(true_tags, predicted_tags):#creates a tuple list\n",
        "\n",
        "                # d. Update tag_stats\n",
        "                # Always increment the total count for the TRUE tag\n",
        "                tag_stats[true_tag]['total'] += 1\n",
        "\n",
        "                # Only increment 'correct' if the prediction matches the truth\n",
        "                if true_tag == pred_tag:\n",
        "                    tag_stats[true_tag]['correct'] += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            # 3. Handle exceptions\n",
        "            # If the tagger crashes on a sentence (e.g., unknown symbol),\n",
        "            # we count every tag in that sentence as a \"miss\" (incorrect).\n",
        "            for tag in true_tags:\n",
        "                tag_stats[tag]['total'] += 1\n",
        "                # We do NOT increment 'correct' here\n",
        "\n",
        "    # 4. Return the result\n",
        "    return dict(tag_stats)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkl012"
      },
      "source": [
        "## 1.4: Semi-supervised HMM Training (20 points)\n",
        "\n",
        "Implement semi-supervised HMM training that combines a small amount of labelled data (1%) with larger unlabelled data (99%).\n",
        "\n",
        "**Steps:**\n",
        "1. Split data: 1% tagged, 99% untagged\n",
        "2. Train supervised model on 1% tagged data\n",
        "3. Use this model to initialise Baum-Welch on 99% untagged data\n",
        "4. Refine with unsupervised learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mno345"
      },
      "outputs": [],
      "source": [
        "def train_semi_supervised(tagged_data, percent_tagged=1.0):\n",
        "    \"\"\"\n",
        "    Train HMM with a mix of tagged and untagged data.\n",
        "\n",
        "    Args:\n",
        "        tagged_data: List of tagged sentences\n",
        "        percent_tagged: Percentage of data to keep tags (default 1.0%)\n",
        "\n",
        "    Returns:\n",
        "        Trained HMM tagger\n",
        "    \"\"\"\n",
        "    # TODO: Implement semi-supervised training\n",
        "    #\n",
        "    # Steps:\n",
        "    # 1. Calculate split index: int(len(tagged_data) * (percent_tagged / 100.0))\n",
        "    #    Ensure at least 1 sentence is tagged\n",
        "    #\n",
        "    # 2. Split data:\n",
        "    #    tagged_portion = tagged_data[:split_idx]\n",
        "    #    untagged_portion = remove_tags(tagged_data[split_idx:])\n",
        "    #\n",
        "    # 3. Extract states and symbols from ALL data\n",
        "    #    NOTE: In fact, we should have used only the tagged_data for this,\n",
        "    #    but we are not sure if that portion includes all the tags and symbols\n",
        "    #\n",
        "    # 4. Initialize trainer with states and symbols using\n",
        "    #    hmm.HiddenMarkovModelTrainer\n",
        "    #\n",
        "    # 5. Train supervised model on small tagged data using trainer.train_supervised\n",
        "    #\n",
        "    # 6. Refine with Baum-Welch on untagged data using trainer.train_unsupervised\n",
        "    #\n",
        "    # 7. Return refined_tagger (or error if no untagged data or no tagged data)\n",
        "\n",
        "# 1. Calculate split index\n",
        "    total_len = len(tagged_data)\n",
        "    split_idx = int(total_len * (percent_tagged / 100.0))\n",
        "\n",
        "    # Safety check: We need at least one labeled sentence to start the process\n",
        "    if split_idx < 1 and total_len > 0:\n",
        "        split_idx = 1\n",
        "\n",
        "    print(f\"Splitting: {split_idx} tagged, {total_len - split_idx} untagged.\")\n",
        "\n",
        "    # 2. Split data\n",
        "    tagged_portion = tagged_data[:split_idx]\n",
        "\n",
        "    # IMPORTANT: We must use the remove_tags function you wrote earlier\n",
        "    # to simulate unlabelled data for the rest of the corpus\n",
        "    untagged_portion = remove_tags(tagged_data[split_idx:])\n",
        "\n",
        "    # 3. Extract states and symbols from ALL data\n",
        "    # We scan the entire dataset (even the parts we will \"untag\" later)\n",
        "    # to ensure the HMM knows every possible word (Symbol) and tag (State) exists.\n",
        "    # If we don't do this, the unsupervised trainer will crash on 'unknown' words.\n",
        "    all_states = set()\n",
        "    all_symbols = set()\n",
        "\n",
        "    for sent in tagged_data:\n",
        "        for word, tag in sent:\n",
        "            all_states.add(tag)\n",
        "            all_symbols.add(word)\n",
        "\n",
        "    # 4. Initialize trainer\n",
        "    # We must explicitly provide the full list of states and symbols\n",
        "    trainer = hmm.HiddenMarkovModelTrainer(\n",
        "        states=list(all_states),\n",
        "        symbols=list(all_symbols)\n",
        "    )\n",
        "\n",
        "    # 5. Train supervised model on the small tagged portion\n",
        "    # This creates our \"seed\" model with rough probabilities\n",
        "    print(\"Step 1: Training initial supervised model...\")\n",
        "    supervised_model = trainer.train_supervised(tagged_portion)\n",
        "\n",
        "    # 6. Refine with Baum-Welch on untagged data\n",
        "    # If we have untagged data, we use the supervised_model to initialize\n",
        "    # the unsupervised training. This is the \"Semi-Supervised\" magic.\n",
        "    if len(untagged_portion) > 0:\n",
        "        print(\"Step 2: Refining with unsupervised learning (this may take time)...\")\n",
        "\n",
        "        # We pass model=supervised_model. This tells NLTK:\n",
        "        # \"Don't start with random guesses. Start with what you learned in Step 1.\"\n",
        "        refined_model = trainer.train_unsupervised(\n",
        "            untagged_portion,\n",
        "            model=supervised_model,\n",
        "            max_iterations= 5 # Optional: limit iterations to save time during debug\n",
        "        )\n",
        "        return refined_model\n",
        "    else:\n",
        "        # If percent_tagged was 100%, just return the supervised model\n",
        "        return supervised_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqr678"
      },
      "source": [
        "## 1.5: 5-Fold Cross-Validation (25 Points)\n",
        "\n",
        "Implement 5-fold cross-validation to train and evaluate all three models. This ensures robust performance estimates.\n",
        "\n",
        "**Steps:**\n",
        "1. Split data into 5 folds\n",
        "2. For each fold:\n",
        "   - Train\n",
        "    - supervised (`trainer.train_supervised()`)\n",
        "    - unsupervised (`trainer.train_unsupervised()`)\n",
        "    - semi-supervised models (`train_semi_supervised()`)\n",
        "   - Evaluate on test fold\n",
        "   - Accumulate results\n",
        "3. Calculate average accuracy across all folds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "stu901",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9685ea7a-aee2-481b-da9a-fca4a9122a9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "FOLD 1\n",
            "============================================================\n",
            "Training set: 4000 sentences\n",
            "Test set: 1000 sentences\n",
            "\n",
            "1. Training Unsupervised HMM (Baum-Welch)...\n",
            "iteration 0 logprob -1190490.3323746119\n",
            "iteration 1 logprob -878083.5835181965\n",
            "iteration 2 logprob -876624.6737700262\n",
            "iteration 3 logprob -875310.256632989\n",
            "iteration 4 logprob -873853.671049464\n",
            "Unsupervised training complete\n",
            "\n",
            "2. Training Supervised HMM...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/nltk/tag/hmm.py:335: RuntimeWarning: overflow encountered in cast\n",
            "  O[i, k] = self._output_logprob(si, self._symbols[k])\n",
            "/usr/local/lib/python3.12/dist-packages/nltk/tag/hmm.py:333: RuntimeWarning: overflow encountered in cast\n",
            "  X[i, j] = self._transitions[si].logprob(self._states[j])\n",
            "/usr/local/lib/python3.12/dist-packages/nltk/tag/hmm.py:363: RuntimeWarning: overflow encountered in cast\n",
            "  O[i, k] = self._output_logprob(si, self._symbols[k])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Supervised training complete\n",
            "\n",
            "3. Training Semi-Supervised HMM (1% tagged, 99% untagged)...\n",
            "Splitting: 40 tagged, 3960 untagged.\n",
            "Step 1: Training initial supervised model...\n",
            "Step 2: Refining with unsupervised learning (this may take time)...\n",
            "iteration 0 logprob -4.484600000000029e+304\n",
            "iteration 1 logprob -6.391436595342868e+289\n",
            "iteration 2 logprob -892037.196691362\n",
            "iteration 3 logprob -865828.7819085391\n",
            "iteration 4 logprob -863749.4419701553\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/nltk/tag/hmm.py:331: RuntimeWarning: overflow encountered in cast\n",
            "  P[i] = self._priors.logprob(si)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Semi-supervised training complete\n",
            "============================================================\n",
            "FOLD 2\n",
            "============================================================\n",
            "Training set: 4000 sentences\n",
            "Test set: 1000 sentences\n",
            "\n",
            "1. Training Unsupervised HMM (Baum-Welch)...\n",
            "iteration 0 logprob -1193534.6787396027\n",
            "iteration 1 logprob -876247.977715901\n",
            "iteration 2 logprob -874801.011138248\n",
            "iteration 3 logprob -873255.0167542782\n",
            "iteration 4 logprob -871345.2619476303\n",
            "Unsupervised training complete\n",
            "\n",
            "2. Training Supervised HMM...\n",
            "Supervised training complete\n",
            "\n",
            "3. Training Semi-Supervised HMM (1% tagged, 99% untagged)...\n",
            "Splitting: 40 tagged, 3960 untagged.\n",
            "Step 1: Training initial supervised model...\n",
            "Step 2: Refining with unsupervised learning (this may take time)...\n",
            "iteration 0 logprob -4.47010000000003e+304\n",
            "iteration 1 logprob -6.2580809189905805e+289\n",
            "iteration 2 logprob -892858.1622283141\n",
            "iteration 3 logprob -863630.3271532262\n",
            "iteration 4 logprob -861515.4034895102\n",
            "Semi-supervised training complete\n",
            "============================================================\n",
            "FOLD 3\n",
            "============================================================\n",
            "Training set: 4000 sentences\n",
            "Test set: 1000 sentences\n",
            "\n",
            "1. Training Unsupervised HMM (Baum-Welch)...\n",
            "iteration 0 logprob -1203455.130430233\n",
            "iteration 1 logprob -885304.8195326665\n",
            "iteration 2 logprob -883723.3612180128\n",
            "iteration 3 logprob -882162.2886050842\n",
            "iteration 4 logprob -880365.545650681\n",
            "Unsupervised training complete\n",
            "\n",
            "2. Training Supervised HMM...\n",
            "Supervised training complete\n",
            "\n",
            "3. Training Semi-Supervised HMM (1% tagged, 99% untagged)...\n",
            "Splitting: 40 tagged, 3960 untagged.\n",
            "Step 1: Training initial supervised model...\n",
            "Step 2: Refining with unsupervised learning (this may take time)...\n",
            "iteration 0 logprob -4.500200000000026e+304\n",
            "iteration 1 logprob -6.286631643633354e+289\n",
            "iteration 2 logprob -899963.4773967701\n",
            "iteration 3 logprob -872749.5772984265\n",
            "iteration 4 logprob -870773.5358361366\n",
            "Semi-supervised training complete\n",
            "============================================================\n",
            "FOLD 4\n",
            "============================================================\n",
            "Training set: 4000 sentences\n",
            "Test set: 1000 sentences\n",
            "\n",
            "1. Training Unsupervised HMM (Baum-Welch)...\n",
            "iteration 0 logprob -1187793.3642650193\n",
            "iteration 1 logprob -877693.0839337939\n",
            "iteration 2 logprob -876647.0889609521\n",
            "iteration 3 logprob -875643.8484486892\n",
            "iteration 4 logprob -874504.9956452305\n",
            "Unsupervised training complete\n",
            "\n",
            "2. Training Supervised HMM...\n",
            "Supervised training complete\n",
            "\n",
            "3. Training Semi-Supervised HMM (1% tagged, 99% untagged)...\n",
            "Splitting: 40 tagged, 3960 untagged.\n",
            "Step 1: Training initial supervised model...\n",
            "Step 2: Refining with unsupervised learning (this may take time)...\n",
            "iteration 0 logprob -4.3465000000000223e+304\n",
            "iteration 1 logprob -6.494338165409531e+289\n",
            "iteration 2 logprob -891895.8666317296\n",
            "iteration 3 logprob -865387.9278862503\n",
            "iteration 4 logprob -863648.3389071992\n",
            "Semi-supervised training complete\n",
            "============================================================\n",
            "FOLD 5\n",
            "============================================================\n",
            "Training set: 4000 sentences\n",
            "Test set: 1000 sentences\n",
            "\n",
            "1. Training Unsupervised HMM (Baum-Welch)...\n",
            "iteration 0 logprob -1196100.5824024514\n",
            "iteration 1 logprob -885189.4443587987\n",
            "iteration 2 logprob -883766.3142187793\n",
            "iteration 3 logprob -882310.7075844639\n",
            "iteration 4 logprob -880589.6739265214\n",
            "Unsupervised training complete\n",
            "\n",
            "2. Training Supervised HMM...\n",
            "Supervised training complete\n",
            "\n",
            "3. Training Semi-Supervised HMM (1% tagged, 99% untagged)...\n",
            "Splitting: 40 tagged, 3960 untagged.\n",
            "Step 1: Training initial supervised model...\n",
            "Step 2: Refining with unsupervised learning (this may take time)...\n",
            "iteration 0 logprob -4.480800000000034e+304\n",
            "iteration 1 logprob -6.275806160539636e+289\n",
            "iteration 2 logprob -899385.3678988081\n",
            "iteration 3 logprob -871363.6907532072\n",
            "iteration 4 logprob -869059.2626432754\n",
            "Semi-supervised training complete\n"
          ]
        }
      ],
      "source": [
        "# Prepare for 5-fold cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
        "\n",
        "# Storage for results across folds\n",
        "results = {\n",
        "    'unsupervised': defaultdict(lambda: {'correct': 0, 'total': 0}),\n",
        "    'supervised': defaultdict(lambda: {'correct': 0, 'total': 0}),\n",
        "    'semi_supervised': defaultdict(lambda: {'correct': 0, 'total': 0})\n",
        "}\n",
        "fold_num = 0\n",
        "\n",
        "# TODO: Complete the cross-validation loop\n",
        "\n",
        "for train_idx, test_idx in kf.split(all_data):#kf splits the all data we define at the beginning\n",
        "    fold_num += 1 #&&&&&&&&&\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"FOLD {fold_num}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Split data to train_data and test_data\n",
        "    # ...\n",
        "    train_data = [all_data[i] for i in train_idx]  #&&&&&&\n",
        "    test_data = [all_data[i] for i in test_idx]\n",
        "\n",
        "\n",
        "    print(f\"Training set: {len(train_data)} sentences\")\n",
        "    print(f\"Test set: {len(test_data)} sentences\")\n",
        "\n",
        "    # Extract all_tags and all_symbols from training data\n",
        "    #  ...\n",
        "    current_states = set()   #&&&&&&&&&&&\n",
        "    current_symbols = set()\n",
        "    for sent in train_data:\n",
        "        for word, tag in sent:\n",
        "            current_states.add(tag)\n",
        "            current_symbols.add(word) # Keep casing as is from data\n",
        "\n",
        "    # Convert to lists for the trainer\n",
        "    list_states = list(current_states)\n",
        "    list_symbols = list(current_symbols)  #&&&&&&&&&&&\n",
        "\n",
        "    # 1. Unsupervised HMM (Baum-Welch)\n",
        "    print(\"\\n1. Training Unsupervised HMM (Baum-Welch)...\")\n",
        "    try:\n",
        "        # 1.1 Convert to untagged format\n",
        "\n",
        "        # 1.2 Initialize trainer with states and symbols\n",
        "\n",
        "        # 1.3 Train unsupervised\n",
        "\n",
        "        # 1.4 Evaluate unsupervised tagger and update the results dict\n",
        "\n",
        "        # 1.1 Convert to untagged format\n",
        "        untagged_train = remove_tags(train_data)\n",
        "\n",
        "        # 1.2 Initialize trainer\n",
        "        trainer_unsup = hmm.HiddenMarkovModelTrainer(states=list_states, symbols=list_symbols)\n",
        "\n",
        "        # 1.3 Train unsupervised (Limit iterations to save time)\n",
        "        # BU SATIR EKSİKTİ, EKLENDİ:\n",
        "        tagger_unsup = trainer_unsup.train_unsupervised(untagged_train, max_iterations=5)\n",
        "\n",
        "        # 1.4 Evaluate\n",
        "        fold_stats = evaluate_tagger(tagger_unsup, test_data)\n",
        "\n",
        "        # Accumulate results\n",
        "        for tag, counts in fold_stats.items():\n",
        "            results['unsupervised'][tag]['correct'] += counts['correct']\n",
        "            results['unsupervised'][tag]['total'] += counts['total']\n",
        "\n",
        "        print(\"Unsupervised training complete\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Unsupervised training failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "    # 2. Supervised HMM\n",
        "    print(\"\\n2. Training Supervised HMM...\")\n",
        "    try:\n",
        "        # 1.1 Initialize trainer with states and symbols\n",
        "        trainer_sup = hmm.HiddenMarkovModelTrainer(states=list_states, symbols=list_symbols)\n",
        "\n",
        "        # 1.2 Train supervised\n",
        "        tagger_sup = trainer_sup.train_supervised(train_data)\n",
        "\n",
        "        # 1.3 Evaluate supervised tagger and update the results dict\n",
        "        fold_stats = evaluate_tagger(tagger_sup, test_data)\n",
        "\n",
        "        # Accumulate results\n",
        "        for tag, counts in fold_stats.items():\n",
        "            results['supervised'][tag]['correct'] += counts['correct']\n",
        "            results['supervised'][tag]['total'] += counts['total']\n",
        "        # 1.1 Initialize trainer with states and symbols\n",
        "\n",
        "        # 1.2 Train supervised\n",
        "\n",
        "        # 1.3 Evaluate supervised tagger and update the results dict\n",
        "\n",
        "        print(\"Supervised training complete\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Supervised training failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "    # 3. Semi-supervised HMM (1% tagged, 99% untagged)\n",
        "    print(\"\\n3. Training Semi-Supervised HMM (1% tagged, 99% untagged)...\")\n",
        "    try:\n",
        "        # 1.1 Train semi-supervised\n",
        "\n",
        "        # 1.3 Evaluate semi-supervised tagger and update the results dict\n",
        "\n",
        "\n",
        "        # 1.1 Train semi-supervised\n",
        "        # We pass the custom function we wrote earlier\n",
        "        # Note: We don't need to manually init the trainer here, the function does it.\n",
        "        tagger_semi_sup = train_semi_supervised(train_data, percent_tagged=1.0)\n",
        "\n",
        "        # 1.3 Evaluate semi-supervised tagger and update the results dict\n",
        "        fold_stats = evaluate_tagger(tagger_semi_sup, test_data)\n",
        "\n",
        "        # Accumulate results\n",
        "        for tag, counts in fold_stats.items():\n",
        "            results['semi_supervised'][tag]['correct'] += counts['correct']\n",
        "            results['semi_supervised'][tag]['total'] += counts['total']\n",
        "\n",
        "        print(\"Semi-supervised training complete\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Semi-supervised training failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZLYOOQPz-Si"
      },
      "source": [
        "**Question 1.1:** In an unsupervised HMM with 12 hidden states, you replace the intended PoS tags with meaningless labels like S1–S12. How would this change affect the model’s learned behaviour and its evaluation accuracy? (4 points, 3-4 sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc9EPblW0JgN"
      },
      "source": [
        "**[YOUR ANSWER HERE]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "question1"
      },
      "source": [
        "**Question 1.2:** You initialize a semi-supervised HMM with a fully supervised model and then run Baum–Welch only on unlabeled data. How can this additional unsupervised training change the model’s behaviour and accuracy? (4 points, 3-4 sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kZZAfPOcRtr"
      },
      "source": [
        "**[YOUR ANSWER HERE]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "question2"
      },
      "source": [
        "**Question 1.3:** Consider the following four changes to an unsupervised or semi-supervised HMM for PoS tagging. For each one, state whether it would make training faster, slower, or have no significant effect, and justify your choice in one sentence. (4 points, 4 sentences)\n",
        "- Reducing the number of hidden states from 12 to 6.\n",
        "- Initializing the model with a fully supervised HMM instead of random parameters.\n",
        "- Increasing the size of the unlabeled corpus by a factor of 5.\n",
        "- Replacing full EM with a fixed small number of EM iterations (e.g., exactly 3 passes)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMu3eWDRcSUd"
      },
      "source": [
        "**[YOUR ANSWER HERE]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwx234"
      },
      "source": [
        "## 1.6: Display Results (5 points)\n",
        "\n",
        "Display the average across folds results in a formatted table showing\n",
        "- tag frequency (%)\n",
        "- per-tag accuracy (%)\n",
        "- overall accuracy (%)\n",
        "\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n",
        "\n",
        "for all three models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "yza567",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7024ce84-95e9-4287-a6fb-51b5d3597830"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "               FINAL EVALUATION RESULTS (Average across 5 Folds)                \n",
            "================================================================================\n",
            "\n",
            "MODEL: SUPERVISED\n",
            "---------------------------------------------\n",
            "TAG          | FREQ (%)     | ACCURACY (%)\n",
            "---------------------------------------------\n",
            ".            |      11.68% |      33.45%\n",
            "ADJ          |       6.82% |      97.67%\n",
            "ADP          |      12.32% |      40.17%\n",
            "ADV          |       3.42% |      43.48%\n",
            "CONJ         |       2.73% |      35.10%\n",
            "DET          |      11.42% |      49.04%\n",
            "NOUN         |      30.17% |      36.18%\n",
            "NUM          |       2.06% |      39.58%\n",
            "PRON         |       2.56% |      56.18%\n",
            "PRT          |       2.25% |      40.83%\n",
            "VERB         |      14.47% |      44.73%\n",
            "X            |       0.09% |      23.23%\n",
            "---------------------------------------------\n",
            "OVERALL      |    100.00% |      44.15%\n",
            "\n",
            "================================================================================\n",
            "\n",
            "MODEL: UNSUPERVISED\n",
            "---------------------------------------------\n",
            "TAG          | FREQ (%)     | ACCURACY (%)\n",
            "---------------------------------------------\n",
            ".            |      11.68% |       1.88%\n",
            "ADJ          |       6.82% |      64.47%\n",
            "ADP          |      12.32% |       2.71%\n",
            "ADV          |       3.42% |       3.33%\n",
            "CONJ         |       2.73% |       5.36%\n",
            "DET          |      11.42% |       4.11%\n",
            "NOUN         |      30.17% |       3.53%\n",
            "NUM          |       2.06% |       4.02%\n",
            "PRON         |       2.56% |       4.92%\n",
            "PRT          |       2.25% |       3.59%\n",
            "VERB         |      14.47% |       6.05%\n",
            "X            |       0.09% |       1.01%\n",
            "---------------------------------------------\n",
            "OVERALL      |    100.00% |       7.91%\n",
            "\n",
            "================================================================================\n",
            "\n",
            "MODEL: SEMI SUPERVISED\n",
            "---------------------------------------------\n",
            "TAG          | FREQ (%)     | ACCURACY (%)\n",
            "---------------------------------------------\n",
            ".            |      11.68% |       3.09%\n",
            "ADJ          |       6.82% |      70.89%\n",
            "ADP          |      12.32% |      17.30%\n",
            "ADV          |       3.42% |       7.50%\n",
            "CONJ         |       2.73% |      15.81%\n",
            "DET          |      11.42% |      12.73%\n",
            "NOUN         |      30.17% |       4.86%\n",
            "NUM          |       2.06% |       7.45%\n",
            "PRON         |       2.56% |      31.14%\n",
            "PRT          |       2.25% |      24.05%\n",
            "VERB         |      14.47% |      17.44%\n",
            "X            |       0.09% |       0.00%\n",
            "---------------------------------------------\n",
            "OVERALL      |    100.00% |      14.95%\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# TODO: Calculate and display results\n",
        "#\n",
        "\n",
        "\n",
        "# Iterate through each model type (Supervised, Unsupervised, Semi-supervised)\n",
        "# We sort the keys to ensure a consistent order, or manually specify the list\n",
        "model_order = ['supervised', 'unsupervised', 'semi_supervised']\n",
        "\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"{'FINAL EVALUATION RESULTS (Average across 5 Folds)':^80}\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "\n",
        "for model_name in model_order:\n",
        "    # Check if we have data for this model\n",
        "    if model_name not in results:\n",
        "        continue\n",
        "\n",
        "    data = results[model_name]\n",
        "\n",
        "    # 1. Calculate Overall Statistics\n",
        "    total_correct = sum(tag_data['correct'] for tag_data in data.values())\n",
        "    total_tokens = sum(tag_data['total'] for tag_data in data.values())\n",
        "\n",
        "    # Avoid division by zero\n",
        "    overall_accuracy = (total_correct / total_tokens * 100) if total_tokens > 0 else 0.0\n",
        "\n",
        "    # 2. Print Header for the Model\n",
        "    print(f\"MODEL: {model_name.upper().replace('_', ' ')}\")\n",
        "    print(f\"{'-'*45}\")\n",
        "    print(f\"{'TAG':<12} | {'FREQ (%)':<12} | {'ACCURACY (%)':<12}\")\n",
        "    print(f\"{'-'*45}\")\n",
        "\n",
        "    # 3. Calculate and Print Per-Tag Statistics\n",
        "    # Sort tags alphabetically for cleaner reading\n",
        "    sorted_tags = sorted(data.keys())\n",
        "\n",
        "    for tag in sorted_tags:\n",
        "        stats = data[tag]\n",
        "        tag_total = stats['total']\n",
        "        tag_correct = stats['correct']\n",
        "\n",
        "        if tag_total > 0:\n",
        "            # Frequency: How often does this tag appear in the whole text?\n",
        "            frequency = (tag_total / total_tokens) * 100\n",
        "\n",
        "            # Accuracy: How often did we get THIS specific tag right?\n",
        "            accuracy = (tag_correct / tag_total) * 100\n",
        "\n",
        "            print(f\"{tag:<12} | {frequency:>10.2f}% | {accuracy:>10.2f}%\")\n",
        "        else:\n",
        "            print(f\"{tag:<12} | {'0.00%':>10} | {'N/A':>10}\")\n",
        "\n",
        "    # 4. Print Overall Accuracy Footer\n",
        "    print(f\"{'-'*45}\")\n",
        "    print(f\"{'OVERALL':<12} | {'100.00%':>10} | {overall_accuracy:>10.2f}%\")\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcd890"
      },
      "source": [
        "## 1.7: Sample Predictions\n",
        "\n",
        "Test the models on sample sentences to see how it performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "efg123",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62affe45-f67c-41e2-c528-4d67523e0ef3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample predictions from last fold:\n",
            "========== Spervised HMM Tagger ==========\n",
            "  Today is a good day .\n",
            "  → [('Today', 'NOUN'), ('is', 'VERB'), ('a', 'DET'), ('good', 'ADJ'), ('day', 'NOUN'), ('.', '.')]\n",
            "\n",
            "  Joe met Joanne in Delhi .\n",
            "  → [('Joe', 'NOUN'), ('met', 'VERB'), ('Joanne', 'NOUN'), ('in', 'ADP'), ('Delhi', 'NOUN'), ('.', '.')]\n",
            "\n",
            "  Time flies like an arrow .\n",
            "  → [('Time', 'NOUN'), ('flies', 'VERB'), ('like', 'ADP'), ('an', 'DET'), ('arrow', 'ADJ'), ('.', 'ADJ')]\n",
            "\n",
            "  The good , the bad , the ugly went to a bar .\n",
            "  → [('The', 'DET'), ('good', 'ADJ'), (',', '.'), ('the', 'DET'), ('bad', 'ADJ'), (',', '.'), ('the', 'DET'), ('ugly', 'ADJ'), ('went', 'VERB'), ('to', 'ADP'), ('a', 'DET'), ('bar', 'NOUN'), ('.', '.')]\n",
            "\n",
            "========== Unspervised HMM Tagger ==========\n",
            "  Today is a good day .\n",
            "  → [('Today', 'CONJ'), ('is', 'ADV'), ('a', 'ADP'), ('good', 'VERB'), ('day', 'PRON'), ('.', 'ADP')]\n",
            "\n",
            "  Joe met Joanne in Delhi .\n",
            "  → [('Joe', 'ADJ'), ('met', 'DET'), ('Joanne', 'CONJ'), ('in', 'ADV'), ('Delhi', 'DET'), ('.', 'ADJ')]\n",
            "\n",
            "  Time flies like an arrow .\n",
            "  → [('Time', 'ADV'), ('flies', 'NUM'), ('like', 'ADP'), ('an', 'VERB'), ('arrow', 'ADJ'), ('.', 'ADJ')]\n",
            "\n",
            "  The good , the bad , the ugly went to a bar .\n",
            "  → [('The', 'ADV'), ('good', 'DET'), (',', 'PRON'), ('the', 'ADP'), ('bad', 'ADP'), (',', 'VERB'), ('the', 'NUM'), ('ugly', '.'), ('went', 'DET'), ('to', 'NOUN'), ('a', 'ADP'), ('bar', 'ADP'), ('.', 'VERB')]\n",
            "\n",
            "========== Semi-spervised HMM Tagger ==========\n",
            "  Today is a good day .\n",
            "  → [('Today', 'DET'), ('is', 'VERB'), ('a', 'CONJ'), ('good', 'ADJ'), ('day', 'PRT'), ('.', 'ADP')]\n",
            "\n",
            "  Joe met Joanne in Delhi .\n",
            "  → [('Joe', 'DET'), ('met', 'PRT'), ('Joanne', 'DET'), ('in', 'NUM'), ('Delhi', 'ADJ'), ('.', 'ADJ')]\n",
            "\n",
            "  Time flies like an arrow .\n",
            "  → [('Time', 'DET'), ('flies', 'ADJ'), ('like', 'PRT'), ('an', 'DET'), ('arrow', 'ADJ'), ('.', 'ADJ')]\n",
            "\n",
            "  The good , the bad , the ugly went to a bar .\n",
            "  → [('The', 'DET'), ('good', 'PRT'), (',', 'ADP'), ('the', 'CONJ'), ('bad', 'ADV'), (',', 'PRON'), ('the', 'CONJ'), ('ugly', 'ADP'), ('went', 'CONJ'), ('to', 'PRON'), ('a', 'CONJ'), ('bar', 'ADV'), ('.', 'ADJ')]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Print sample predictions from the last fold\n",
        "print(\"Sample predictions from last fold:\")\n",
        "sample_sentences = [\n",
        "    \"Today is a good day .\",\n",
        "    \"Joe met Joanne in Delhi .\",\n",
        "    \"Time flies like an arrow .\",\n",
        "    \"The good , the bad , the ugly went to a bar .\"\n",
        "]\n",
        "\n",
        "print(f\"{'='*10} Spervised HMM Tagger {'='*10}\")\n",
        "for sent in sample_sentences:\n",
        "    try:\n",
        "        tagged = tagger_sup.tag(sent.split())\n",
        "        print(f\"  {sent}\")\n",
        "        print(f\"  → {tagged}\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"  {sent}\")\n",
        "        print(f\"ERROR: Tagging failed: {e}\\n\")\n",
        "\n",
        "print(f\"{'='*10} Unspervised HMM Tagger {'='*10}\")\n",
        "for sent in sample_sentences:\n",
        "    try:\n",
        "        tagged = tagger_unsup.tag(sent.split())\n",
        "        print(f\"  {sent}\")\n",
        "        print(f\"  → {tagged}\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"  {sent}\")\n",
        "        print(f\"ERROR: Tagging failed: {e}\\n\")\n",
        "\n",
        "print(f\"{'='*10} Semi-spervised HMM Tagger {'='*10}\")\n",
        "for sent in sample_sentences:\n",
        "    try:\n",
        "        tagged = tagger_semi_sup.tag(sent.split())\n",
        "        print(f\"  {sent}\")\n",
        "        print(f\"  → {tagged}\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"  {sent}\")\n",
        "        print(f\"ERROR: Tagging failed: {e}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part2_header"
      },
      "source": [
        "---\n",
        "\n",
        "# Task 2: Text Generation with HMMs (24 points)\n",
        "\n",
        "In this part, you will use a trained HMM to generate text. The idea is to sample from the learned transition and emission probabilities to create new sequences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train_gen_model"
      },
      "source": [
        "## 2.1: Train HMM Model for Generation\n",
        "\n",
        "First, train a supervised HMM model on the Brown corpus for text generation. We'll normalise words to lowercase for better generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "train_model_gen",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ad1465b-7a13-41c6-f8d2-772165b76dcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Brown corpus...\n",
            "Training HMM on 20000 sentences...\n",
            "Training complete!\n",
            "\n",
            "Model Statistics:\n",
            "  - Number of states (POS tags): 12\n",
            "  - Number of symbols (words): 30743\n",
            "  - States: ., ADJ, ADP, ADV, CONJ, DET, NOUN, NUM, PRON, PRT, VERB, X\n"
          ]
        }
      ],
      "source": [
        "def train_hmm_model(num_sentences=5000):\n",
        "    \"\"\"Train an HMM model on Brown corpus for text generation\"\"\"\n",
        "    print(\"Loading Brown corpus...\")\n",
        "    tagged_sents = list(brown.tagged_sents(tagset=\"universal\"))[:num_sentences]\n",
        "\n",
        "    print(f\"Training HMM on {len(tagged_sents)} sentences...\")\n",
        "\n",
        "    # Extract states and symbols\n",
        "    all_tags = set()\n",
        "    all_symbols = set()\n",
        "    for sent in tagged_sents:\n",
        "        for word, tag in sent:\n",
        "            all_tags.add(tag)\n",
        "            all_symbols.add(word.lower())  # Normalise to lowercase\n",
        "\n",
        "    # Normalise training data to lowercase\n",
        "    normalized_sents = []\n",
        "    for sent in tagged_sents:\n",
        "        normalized_sents.append([(word.lower(), tag) for word, tag in sent])\n",
        "\n",
        "    # Train the model\n",
        "    trainer = hmm.HiddenMarkovModelTrainer(\n",
        "        states=list(all_tags),\n",
        "        symbols=list(all_symbols)\n",
        "    )\n",
        "\n",
        "    tagger = trainer.train_supervised(normalized_sents)\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "    return tagger\n",
        "\n",
        "# Train the model\n",
        "tagger_gen = train_hmm_model(num_sentences=20000)\n",
        "\n",
        "# Show model statistics\n",
        "print(f\"\\nModel Statistics:\")\n",
        "print(f\"  - Number of states (POS tags): {len(tagger_gen._states)}\")\n",
        "print(f\"  - Number of symbols (words): {len(tagger_gen._symbols)}\")\n",
        "print(f\"  - States: {', '.join(sorted(tagger_gen._states))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sample_state_header"
      },
      "source": [
        "## 2.2: Implement State Sampling (7 points)\n",
        "\n",
        "Implement the `sample_state()` function that samples the next POS tag based on transition probabilities.\n",
        "\n",
        "**Temperature parameter:** Controls randomness\n",
        "- Lower temperature (e.g., 0.5): More conservative, follows high-probability transitions\n",
        "- Higher temperature (e.g., 2.0): More creative, explores diverse transitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "sample_state_code"
      },
      "outputs": [],
      "source": [
        "def sample_state(tagger, current_state, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Sample next state from transition distribution.\n",
        "\n",
        "    Args:\n",
        "        tagger: Trained HMM tagger\n",
        "        current_state: Current POS tag\n",
        "        temperature: Controls randomness (default 1.0)\n",
        "\n",
        "    Returns:\n",
        "        Next POS tag (state)\n",
        "    \"\"\"\n",
        "    # TODO: Implement state sampling\n",
        "    #\n",
        "    # Steps:\n",
        "    # 1. Get all possible states from tagger._states\n",
        "    #\n",
        "    # 2. For each state, get transition probability from current_state\n",
        "    #    using: tagger._transitions[current_state].logprob(state)\n",
        "    #\n",
        "    # 3. Apply temperature scaling: probs = probs / temperature\n",
        "    #\n",
        "    # 4. Convert from log2 probabilities to regular probabilities: 2 ** probs\n",
        "    #\n",
        "    # 5. Normalise probabilities to sum to 1\n",
        "    #\n",
        "    # 6. Sample using np.random.choice(states, p=probs)\n",
        "\n",
        "    # 1. Get all possible states from tagger._states\n",
        "    # We convert to a list to ensure consistent ordering for numpy\n",
        "    states = list(tagger._states)\n",
        "\n",
        "    # 2. Get transition probabilities (Log Base 2)\n",
        "    # We use a list comprehension to get the probability of moving from\n",
        "    # current_state -> s for every possible state 's'\n",
        "    log_probs = np.array([tagger._transitions[current_state].logprob(s) for s in states])\n",
        "\n",
        "    # 3. Apply temperature scaling\n",
        "    # We divide the log probs by temperature.\n",
        "    # Avoid division by zero by setting a lower bound if needed,\n",
        "    # though usually we assume temp > 0.\n",
        "    if temperature <= 0: temperature = 1e-10\n",
        "    scaled_log_probs = log_probs / temperature\n",
        "\n",
        "    # 4. Convert from log2 probabilities to regular probabilities\n",
        "    # NLTK uses log base 2, so we calculate 2 to the power of x\n",
        "    probs = 2.0 ** scaled_log_probs\n",
        "\n",
        "    # 5. Normalise probabilities to sum to 1\n",
        "    total_prob = np.sum(probs)\n",
        "\n",
        "    # Handle edge case: if current_state has no valid transitions (sum is 0),\n",
        "    # fall back to uniform distribution to prevent crash\n",
        "    if total_prob == 0:\n",
        "        probs = np.ones(len(states)) / len(states)\n",
        "    else:\n",
        "        probs = probs / total_prob\n",
        "\n",
        "    # 6. Sample using np.random.choice\n",
        "    next_state = np.random.choice(states, p=probs)\n",
        "\n",
        "    return next_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sample_word_header"
      },
      "source": [
        "## 2.3: Implement Word Sampling (7 points)\n",
        "\n",
        "Implement the `sample_word()` function that samples a word given a POS tag based on emission probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "sample_word_code"
      },
      "outputs": [],
      "source": [
        "def sample_word(tagger, state, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Sample word from output distribution of given state.\n",
        "\n",
        "    Args:\n",
        "        tagger: Trained HMM tagger\n",
        "        state: Current POS tag\n",
        "        temperature: Controls randomness (default 1.0)\n",
        "\n",
        "    Returns:\n",
        "        Sampled word\n",
        "    \"\"\"\n",
        "    # TODO: Implement word sampling\n",
        "    #\n",
        "    # Steps:\n",
        "    # 1. Get all possible symbols (words) from tagger._symbols\n",
        "    #\n",
        "    # 2. For each symbol, get emission probability from state\n",
        "    #    using: tagger._outputs[state].logprob(symbol)\n",
        "    #\n",
        "    # 3. Apply temperature scaling: probs = probs / temperature\n",
        "    #\n",
        "    # 4. Convert from log2 probabilities to regular probabilities: 2 ** probs\n",
        "    #\n",
        "    # 5. Handle infinities (words with zero probability) using np.nan_to_num\n",
        "    #\n",
        "    # 6. Normalise probabilities to sum to 1\n",
        "    #    (if all are zero, use uniform distribution)\n",
        "    #\n",
        "    # 7. Sample using np.random.choice(symbols, p=probs)\n",
        "\n",
        "# 1. Get all possible symbols (words) from tagger._symbols\n",
        "    # Convert to list to ensure index alignment with probabilities\n",
        "    symbols = list(tagger._symbols)\n",
        "\n",
        "    # 2. Get emission probabilities (Log Base 2)\n",
        "    # tagger._outputs[state] gives the probability distribution for that state.\n",
        "    # We calculate the log probability for every word in the vocabulary.\n",
        "    log_probs = np.array([tagger._outputs[state].logprob(word) for word in symbols])\n",
        "\n",
        "    # 3. Apply temperature scaling\n",
        "    if temperature <= 0: temperature = 1e-10\n",
        "    scaled_log_probs = log_probs / temperature\n",
        "\n",
        "    # 4. Convert from log2 probabilities to regular probabilities\n",
        "    # 2^x converts NLTK's log2 notation back to standard probability space\n",
        "    probs = 2.0 ** scaled_log_probs\n",
        "\n",
        "    # 5. Handle infinities and NaNs\n",
        "    # Words with 0 probability have log_prob of -inf.\n",
        "    # 2**-inf is 0, but sometimes numpy generates nan or very small garbage values.\n",
        "    # This cleans the array.\n",
        "    probs = np.nan_to_num(probs)\n",
        "\n",
        "    # 6. Normalise probabilities to sum to 1\n",
        "    total_prob = np.sum(probs)\n",
        "\n",
        "    if total_prob == 0:\n",
        "        # If the state has no emissions (unlikely but possible in broken models),\n",
        "        # return a random word to avoid crashing\n",
        "        probs = np.ones(len(symbols)) / len(symbols)\n",
        "    else:\n",
        "        probs = probs / total_prob\n",
        "\n",
        "    # 7. Sample using np.random.choice\n",
        "    word = np.random.choice(symbols, p=probs)\n",
        "\n",
        "    return word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gen_text_header"
      },
      "source": [
        "## 2.4: Implement Text Generation (10 points)\n",
        "\n",
        "Implement the `generate_text_from_word()` function that generates text starting from a given word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "gen_text_code"
      },
      "outputs": [],
      "source": [
        "def generate_text_from_word(tagger, start_word, length=20, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Generate text starting with a given word using the HMM model.\n",
        "\n",
        "    Args:\n",
        "        tagger: Trained HMM tagger\n",
        "        start_word: Word to start the generation\n",
        "        length: Number of words to generate\n",
        "        temperature: Controls randomness (higher = more random)\n",
        "\n",
        "    Returns:\n",
        "        List of generated words\n",
        "    \"\"\"\n",
        "    # TODO: Implement text generation\n",
        "    #\n",
        "    # Steps:\n",
        "    # 1. Normalise start_word to lowercase\n",
        "    #\n",
        "    # 2. Check if start_word is in vocabulary (tagger._symbols)\n",
        "    #    - If not, find similar words or use random word\n",
        "    #\n",
        "    # 3. Initialise generated list with start_word\n",
        "    #\n",
        "    # 4. Infer the most likely tag for start_word:\n",
        "    #    - For each state, get emission probability\n",
        "    #    - Choose state with highest probability\n",
        "    #\n",
        "    # 5. Generate subsequent words in a loop:\n",
        "    #    - Sample next state using sample_state()\n",
        "    #    - Sample word from that state using sample_word()\n",
        "    #    - Append word to generated list\n",
        "    #    - Update current_state\n",
        "    #\n",
        "    # 6. Return generated list\n",
        "\n",
        "# 1. Normalise start_word to lowercase\n",
        "    # This matches the training data format\n",
        "    current_word = start_word.lower()\n",
        "\n",
        "    # 2. Check if start_word is in vocabulary\n",
        "    # We convert to a set for faster lookup, though list check works too\n",
        "    vocab = set(tagger._symbols)\n",
        "\n",
        "    if current_word not in vocab:\n",
        "        print(f\"Warning: '{current_word}' not in vocabulary. Picking a random start word.\")\n",
        "        # Fallback: Pick a random word from the known vocabulary\n",
        "        current_word = random.choice(list(vocab))\n",
        "\n",
        "    # 3. Initialise generated list\n",
        "    generated_words = [current_word]\n",
        "\n",
        "    # 4. Infer the most likely tag (state) for the start_word\n",
        "    # We need to know \"What Part of Speech is this word?\" to start the chain.\n",
        "    # We check P(Word | Tag) for every tag and pick the highest one.\n",
        "    best_state = None\n",
        "    max_log_prob = -float('inf')\n",
        "\n",
        "    for state in tagger._states:\n",
        "        # Get emission probability: P(word | state)\n",
        "        # Note: logprob returns -inf if probability is 0\n",
        "        prob = tagger._outputs[state].logprob(current_word)\n",
        "\n",
        "        if prob > max_log_prob:\n",
        "            max_log_prob = prob\n",
        "            best_state = state\n",
        "\n",
        "    # Safety fallback if something goes wrong (e.g. word has 0 prob in all states)\n",
        "    if best_state is None:\n",
        "        best_state = random.choice(list(tagger._states))\n",
        "\n",
        "    current_state = best_state\n",
        "\n",
        "    # 5. Generate subsequent words in a loop\n",
        "    # We start range at 1 because we already have the 0th word\n",
        "    for _ in range(length - 1):\n",
        "\n",
        "        # A. Sample next state (Transition: State -> State)\n",
        "        next_state = sample_state(tagger, current_state, temperature)\n",
        "\n",
        "        # B. Sample word from that state (Emission: State -> Word)\n",
        "        next_word = sample_word(tagger, next_state, temperature)\n",
        "\n",
        "        # C. Append and Update\n",
        "        generated_words.append(next_word)\n",
        "        current_state = next_state\n",
        "\n",
        "    # 6. Return generated list\n",
        "    return generated_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test_gen_header"
      },
      "source": [
        "## 2.6: Test Text Generation\n",
        "\n",
        "Now test your text generation functions with different starting words and temperatures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "test_gen_code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f05612ab-6cea-4f2e-ab84-fe9352addca5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "EXAMPLE GENERATIONS\n",
            "======================================================================\n",
            "\n",
            "--- Starting with 'the' ---\n",
            "the one , but patriotic office when such pier by investigation postulated from 9 by\n",
            "\n",
            "--- Starting with 'dog' ---\n",
            "dog receiving his less , as the grandiose important borden by the delivering ; a\n",
            "\n",
            "--- Starting with 'running' ---\n",
            "running readily good president . he of deep venture vocational accumulation `` if the sedition\n",
            "\n",
            "--- Starting with 'beautiful' ---\n",
            "beautiful view , , however am being the larger least russia . alarm as generation\n",
            "\n",
            "--- Starting with 'computer' ---\n",
            "computer , a wards , is if hotel ( eclectic file to quarrel the berkman\n",
            "\n",
            "--- Starting with 'yesterday' ---\n",
            "yesterday . bed on the perilous to time . an industrial father scuttled inside in\n"
          ]
        }
      ],
      "source": [
        "# Test with example words\n",
        "print(\"=\"*70)\n",
        "print(\"EXAMPLE GENERATIONS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "example_words = [\"the\", \"dog\", \"running\", \"beautiful\", \"computer\", \"yesterday\"]\n",
        "\n",
        "for word in example_words:\n",
        "    print(f\"\\n--- Starting with '{word}' ---\")\n",
        "    try:\n",
        "        words = generate_text_from_word(tagger_gen, word, length=15, temperature=1.0)\n",
        "        print(\" \".join(words))\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "test_temp_code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0af565e-1b40-42f3-f7e4-e829244fe557"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "TEMPERATURE COMPARISON\n",
            "======================================================================\n",
            "\n",
            "Conservative (temp=0.5):\n",
            "the year be the way in the most berlin . very is is the president days for the home to\n",
            "\n",
            "Balanced (temp=1.0):\n",
            "the plane act , states tests march as political harold signs lines that highwayman sustain but a nov. and grave\n",
            "\n",
            "Creative (temp=2.0):\n",
            "the well water borax , '' yet visual sold-out lebanese out rebel amply a ferdinando in any , of ration\n"
          ]
        }
      ],
      "source": [
        "# Test different temperatures\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TEMPERATURE COMPARISON\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "test_word = \"the\"\n",
        "temperatures = [(\"Conservative\", 0.5), (\"Balanced\", 1.0), (\"Creative\", 2.0)]\n",
        "\n",
        "for temp_name, temp_value in temperatures:\n",
        "    print(f\"\\n{temp_name} (temp={temp_value}):\")\n",
        "    words = generate_text_from_word(tagger_gen, test_word, length=20, temperature=temp_value)\n",
        "    print(\" \".join(words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "question3"
      },
      "source": [
        "**Question 2.1:** How does the temperature parameter affect the quality and creativity of generated text? Provide two specific examples from your outputs. (4 points, 5-6 sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "answer3"
      },
      "source": [
        "**[YOUR ANSWER HERE]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdf_header"
      },
      "source": [
        "# Convert Your Colab Notebook to PDF\n",
        "\n",
        "### Step 1: Download Your Notebook\n",
        "- Go to **File → Download → Download .ipynb**\n",
        "- Save the file to your computer\n",
        "\n",
        "### Step 2: Upload to Colab\n",
        "- Click the **📁 folder icon** on the left sidebar\n",
        "- Click the **upload button**\n",
        "- Select your downloaded .ipynb file\n",
        "- Wait for the upload to complete\n",
        "\n",
        "### Step 3: Run the Code Below\n",
        "- **Uncomment the cell below** and run the cell\n",
        "- This will take about 1-2 minutes to install required packages\n",
        "\n",
        "### Step 4: Enter Notebook Name\n",
        "- When prompted, type your notebook name (e.g.`gs_000000_as2.ipynb`)\n",
        "- Press Enter\n",
        "\n",
        "### The PDF will automatically download to your computer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "pdf_code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "outputId": "ac3fc9ac-3daf-48be-c64e-a6bcc63e230f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing PDF converter... please wait...\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "\n",
            "==================================================\n",
            "COLAB NOTEBOOK TO PDF CONVERTER\n",
            "==================================================\n",
            "\n",
            "STEP 1: Download your notebook\n",
            "- Go to File → Download → Download .ipynb\n",
            "- Save it to your computer\n",
            "\n",
            "STEP 2: Upload it here\n",
            "- Click the folder icon on the left (📁)\n",
            "- Click the upload button and select your .ipynb file\n",
            "- Wait for upload to complete\n",
            "\n",
            "STEP 3: Enter the filename below\n",
            "==================================================\n",
            "\n",
            "Enter your notebook name: cng463_assignment3\n",
            "\n",
            "✓ Found cng463_assignment3.ipynb\n",
            "Converting to PDF... this may take 1-2 minutes...\n",
            "\n",
            "[NbConvertApp] Converting notebook /content/cng463_assignment3.ipynb to pdf\n",
            "[NbConvertApp] Writing 118154 bytes to notebook.tex\n",
            "[NbConvertApp] Building PDF\n",
            "[NbConvertApp] Running xelatex 3 times: ['xelatex', 'notebook.tex', '-quiet']\n",
            "[NbConvertApp] Running bibtex 1 time: ['bibtex', 'notebook']\n",
            "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
            "[NbConvertApp] PDF successfully created\n",
            "[NbConvertApp] Writing 127870 bytes to /content/cng463_assignment3.pdf\n",
            "✓ SUCCESS! Downloading your PDF now...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3732b17d-905f-49c1-950f-49d0a0b909a2\", \"cng463_assignment3.pdf\", 127870)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Done! Check your downloads folder.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages (this takes about 30 seconds)\n",
        "print(\"Installing PDF converter... please wait...\")\n",
        "!apt-get update -qq\n",
        "!apt-get install -y texlive-xetex texlive-fonts-recommended texlive-plain-generic pandoc > /dev/null 2>&1\n",
        "!pip install -q nbconvert\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"COLAB NOTEBOOK TO PDF CONVERTER\")\n",
        "print(\"=\"*50)\n",
        "print(\"\\nSTEP 1: Download your notebook\")\n",
        "print(\"- Go to File → Download → Download .ipynb\")\n",
        "print(\"- Save it to your computer\")\n",
        "print(\"\\nSTEP 2: Upload it here\")\n",
        "print(\"- Click the folder icon on the left (📁)\")\n",
        "print(\"- Click the upload button and select your .ipynb file\")\n",
        "print(\"- Wait for upload to complete\")\n",
        "print(\"\\nSTEP 3: Enter the filename below\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Get notebook name from user\n",
        "notebook_name = input(\"\\nEnter your notebook name: \")\n",
        "\n",
        "# Add .ipynb if missing\n",
        "if not notebook_name.endswith('.ipynb'):\n",
        "    notebook_name += '.ipynb'\n",
        "\n",
        "import os\n",
        "notebook_path = f'/content/{notebook_name}'\n",
        "\n",
        "# Check if file exists\n",
        "if not os.path.exists(notebook_path):\n",
        "    print(f\"\\n⚠ Error: '{notebook_name}' not found in /content/\")\n",
        "    print(\"\\nMake sure you uploaded the file using the folder icon (📁) on the left!\")\n",
        "else:\n",
        "    print(f\"\\n✓ Found {notebook_name}\")\n",
        "    print(\"Converting to PDF... this may take 1-2 minutes...\\n\")\n",
        "\n",
        "    # Convert the notebook to PDF\n",
        "    !jupyter nbconvert --to pdf \"{notebook_path}\"\n",
        "\n",
        "    # Download the PDF\n",
        "    from google.colab import files\n",
        "    pdf_name = notebook_name.replace('.ipynb', '.pdf')\n",
        "    pdf_path = f'/content/{pdf_name}'\n",
        "\n",
        "    if os.path.exists(pdf_path):\n",
        "        print(\"✓ SUCCESS! Downloading your PDF now...\")\n",
        "        files.download(pdf_path)\n",
        "        print(\"\\n✓ Done! Check your downloads folder.\")\n",
        "    else:\n",
        "        print(\"⚠ Error: Could not create PDF\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}